{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import struct\n",
    "import time as time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import PIL.Image\n",
    "from scipy import stats\n",
    "from itertools import chain\n",
    "from scipy.io import loadmat\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from itertools import zip_longest\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neurogen.numpy_utility as pnu\n",
    "from neurogen.plots import display_candidate_loss\n",
    "from neurogen.file_utility import save_stuff, flatten_dict, embed_dict\n",
    "from neurogen.torch_fwrf import get_value\n",
    "\n",
    "from neurogen.torch_fwrf import learn_params_ridge_regression, get_predictions, Torch_fwRF_voxel_block\n",
    "from neurogen.encoding import load_encoding\n",
    "from neurogen.visualize import center_crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths\n",
    "\n",
    "nsd_root = \"data/nsd/\"\n",
    "# stim_root = \"/home/hhan228/memorability/Mansoure/\" + \"nsd_stimuli/\"\n",
    "# beta_root = nsd_root + \"nsd_beta/\"\n",
    "#mask_root = nsd_root + \"mask/ppdata/\"\n",
    "#roi_root = nsd_root + \"freesurfer/\"\n",
    "meanROIbeta_root = nsd_root + \"roiavgbeta_neurogen/\"\n",
    "# weight_root = \"neurogen/output/\"\n",
    "weight_base_dir = \"/home/hhan228/memorability/Willow/neurogen_output/\"\n",
    "# weight_root = weight_base_dir+\"alexnet/\"\n",
    "weight_root = weight_base_dir+\"resnet/\"\n",
    "\n",
    "exp_design_file = nsd_root + \"nsd_expdesign.mat\"\n",
    "stim_file = nsd_root + \"shared1000_original.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With memorability-controlled images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROIs = [\n",
    "    'OFA', 'FFA1', 'FFA2', 'mTLfaces', 'aTLfaces',\n",
    "    'EBA', 'FBA1', 'FBA2', 'mTLbodies',\n",
    "    'OPA', 'PPA', 'RSC',\n",
    "    'V1v', 'V1d', 'V2v', 'V2d', 'V3v', 'V3d', 'hV4',\n",
    "    'L-hippocampus', 'L-amygdala', 'R-hippocampus', 'R-amygdala'\n",
    "]\n",
    "trials = np.array([30000, 30000, 24000, 22500, 30000, 24000, 30000, 22500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr = 0.5\n",
    "base_dir = f\"data/synthetic_new_categories_ver3_{thr}/\"\n",
    "# categories = [\"animals\", \"foods\", \"landscapes\", \"vehicles\"]\n",
    "categories = [\"animals\", \"foods\", \"humans\", \"places\"]\n",
    "controlled = [\"original\", \"increased\", \"decreased\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "subject = 8\n",
    "savearg = {'format':'png', 'dpi': 120, 'facecolor': None}\n",
    "model_name = 'dnn_fwrf'\n",
    "\n",
    "data_size = trials[subject-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_num = len(ROIs)\n",
    "roi_data = np.zeros([data_size, roi_num])\n",
    "n = 0\n",
    "del_idx = []\n",
    "for roi in ROIs:\n",
    "    roi_data[:,n] = np.genfromtxt(meanROIbeta_root + f'subj{subject:02d}/meanbeta_' + roi + '.txt')\n",
    "    if np.isnan(np.sum(roi_data[:,n])):\n",
    "        print(roi)\n",
    "        del_idx.append(n)\n",
    "    n += 1\n",
    "\n",
    "ROIs_bool = np.ones((23,1), dtype='bool')\n",
    "ROIs_bool[del_idx] = False\n",
    "\n",
    "roi_data = np.delete(roi_data, del_idx, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load params\n",
    "model_params_set = h5py.File(weight_root+f'S{subject:02d}/model_params.h5py' , 'r')\n",
    "# model_params_set = h5py.File(weight_root+f'S{subject:02d}/dnn_fwrf/model_params.h5py' , 'r')\n",
    "model_params = embed_dict({k: np.copy(d) for k,d in model_params_set.items()})\n",
    "model_params_set.close()\n",
    "\n",
    "# load encoding models\n",
    "fwrf, fmaps = load_encoding(model_params, fmap_name='resnet', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples [  100:199  ] of 200, voxels [     0:22    ] of 23\n",
      "---------------------------------------\n",
      "total time = 0.088387s\n",
      "sample throughput = 0.000442s/sample\n",
      "voxel throughput = 0.003843s/voxel\n",
      "samples [  100:199  ] of 200, voxels [     0:22    ] of 23\n",
      "---------------------------------------\n",
      "total time = 0.091573s\n",
      "sample throughput = 0.000458s/sample\n",
      "voxel throughput = 0.003981s/voxel\n",
      "samples [  100:199  ] of 200, voxels [     0:22    ] of 23\n",
      "---------------------------------------\n",
      "total time = 0.091424s\n",
      "sample throughput = 0.000457s/sample\n",
      "voxel throughput = 0.003975s/voxel\n",
      "samples [  100:199  ] of 200, voxels [     0:22    ] of 23\n",
      "---------------------------------------\n",
      "total time = 0.091380s\n",
      "sample throughput = 0.000457s/sample\n",
      "voxel throughput = 0.003973s/voxel\n",
      "samples [  100:199  ] of 200, voxels [     0:22    ] of 23\n",
      "---------------------------------------\n",
      "total time = 0.091217s\n",
      "sample throughput = 0.000456s/sample\n",
      "voxel throughput = 0.003966s/voxel\n",
      "samples [  100:199  ] of 200, voxels [     0:22    ] of 23\n",
      "---------------------------------------\n",
      "total time = 0.089946s\n",
      "sample throughput = 0.000450s/sample\n",
      "voxel throughput = 0.003911s/voxel\n",
      "samples [  100:199  ] of 200, voxels [     0:22    ] of 23\n",
      "---------------------------------------\n",
      "total time = 0.087673s\n",
      "sample throughput = 0.000438s/sample\n",
      "voxel throughput = 0.003812s/voxel\n",
      "samples [  100:199  ] of 200, voxels [     0:22    ] of 23\n",
      "---------------------------------------\n",
      "total time = 0.087600s\n",
      "sample throughput = 0.000438s/sample\n",
      "voxel throughput = 0.003809s/voxel\n",
      "samples [  100:199  ] of 200, voxels [     0:22    ] of 23\n",
      "---------------------------------------\n",
      "total time = 0.087766s\n",
      "sample throughput = 0.000439s/sample\n",
      "voxel throughput = 0.003816s/voxel\n",
      "samples [  100:199  ] of 200, voxels [     0:22    ] of 23\n",
      "---------------------------------------\n",
      "total time = 0.087790s\n",
      "sample throughput = 0.000439s/sample\n",
      "voxel throughput = 0.003817s/voxel\n",
      "samples [  100:199  ] of 200, voxels [     0:22    ] of 23\n",
      "---------------------------------------\n",
      "total time = 0.087668s\n",
      "sample throughput = 0.000438s/sample\n",
      "voxel throughput = 0.003812s/voxel\n",
      "samples [  100:199  ] of 200, voxels [     0:22    ] of 23\n",
      "---------------------------------------\n",
      "total time = 0.087730s\n",
      "sample throughput = 0.000439s/sample\n",
      "voxel throughput = 0.003814s/voxel\n"
     ]
    }
   ],
   "source": [
    "for cat in categories:\n",
    "    for ctrld in controlled:\n",
    "        image_data = np.load(base_dir+f\"{cat}/{ctrld}_imgs_200.npy\")\n",
    "        image_data = image_data.astype(np.float32) / 255.\n",
    "        # predict brain response\n",
    "        voxel_pred = get_predictions(image_data, fmaps, fwrf, model_params['params'])\n",
    "        pred_act = np.zeros((200,23))\n",
    "        pred_act[:,ROIs_bool[:,0]] = voxel_pred\n",
    "        save_dir = base_dir+f\"{cat}/predicted_responses/\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        np.save(save_dir+f\"S{subject:02d}_{ctrld}_responses.npy\", pred_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cat in categories:\n",
    "#     cat_dir = base_dir+f\"{cat}/\"\n",
    "#     original_imgs = sorted([f for f in os.listdir(cat_dir+\"original/\") if os.path.isfile(os.path.join(cat_dir+\"original/\", f))])\n",
    "#     increased_imgs = sorted([f for f in os.listdir(cat_dir+\"increased/\") if os.path.isfile(os.path.join(cat_dir+\"increased/\", f))])\n",
    "#     decreased_imgs = sorted([f for f in os.listdir(cat_dir+\"decreased/\") if os.path.isfile(os.path.join(cat_dir+\"decreased/\", f))])\n",
    "    \n",
    "#     if cat != \"vehicles\":\n",
    "#         np.random.seed(42)\n",
    "#         sampled_img_ids = sorted([f.split(\"_memcoef\")[0] for f in np.random.choice(original_imgs, 97, replace=False)])\n",
    "#         original_imgs = sorted([f for f in original_imgs if f.split(\"_memcoef\")[0] in sampled_img_ids])\n",
    "#         increased_imgs = sorted([f for f in increased_imgs if f.split(\"_memcoef\")[0] in sampled_img_ids])\n",
    "#         decreased_imgs = sorted([f for f in decreased_imgs if f.split(\"_memcoef\")[0] in sampled_img_ids])\n",
    "\n",
    "#     orig_imgs = []\n",
    "#     inc_imgs = []\n",
    "#     dec_imgs = []\n",
    "#     for orig_f, inc_f, dec_f in zip(original_imgs, increased_imgs, decreased_imgs):\n",
    "#         orig_imgs.append(np.array(PIL.Image.open(cat_dir+f\"original/{orig_f}\").convert(\"RGB\")).transpose(2, 0, 1))\n",
    "#         inc_imgs.append(np.array(PIL.Image.open(cat_dir+f\"increased/{inc_f}\").convert(\"RGB\")).transpose(2, 0, 1))\n",
    "#         dec_imgs.append(np.array(PIL.Image.open(cat_dir+f\"decreased/{dec_f}\").convert(\"RGB\")).transpose(2, 0, 1))\n",
    "\n",
    "#     orig_imgs = np.array(orig_imgs)\n",
    "#     inc_imgs = np.array(inc_imgs)\n",
    "#     dec_imgs = np.array(dec_imgs)\n",
    "\n",
    "#     print(cat.upper())\n",
    "#     print(f\"orig_imgs: {orig_imgs.shape}\")\n",
    "#     print(f\"inc_imgs: {inc_imgs.shape}\")\n",
    "#     print(f\"dec_imgs: {dec_imgs.shape}\\n\")\n",
    "\n",
    "#     np.save(cat_dir+\"original_imgs_97.npy\", orig_imgs)\n",
    "#     np.save(cat_dir+\"increased_imgs_97.npy\", inc_imgs)\n",
    "#     np.save(cat_dir+\"decreased_imgs_97.npy\", dec_imgs)\n",
    "\n",
    "# def resize_image_tensor(x, newsize):\n",
    "#     tt = x.transpose((0,2,3,1))\n",
    "#     r  = np.ndarray(shape=x.shape[:1]+newsize+(x.shape[1],), dtype=tt.dtype)\n",
    "#     for i,t in enumerate(tt):\n",
    "#         r[i] = np.asarray(PIL.Image.fromarray(t).resize(newsize, resample=PIL.Image.BILINEAR))\n",
    "#     return r.transpose((0,3,1,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
